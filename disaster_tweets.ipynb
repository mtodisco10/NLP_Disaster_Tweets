{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "sample = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4342</td>\n",
       "      <td>0.57034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3271</td>\n",
       "      <td>0.42966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  percentage\n",
       "0   4342     0.57034\n",
       "1   3271     0.42966"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'count': train.target.value_counts(), \n",
    "              'percentage': train.target.value_counts(normalize=True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x128657e10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"target\"].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "give keyords distinct numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_keywords(series):\n",
    "    mapper = {}\n",
    "    u_series = series.unique()\n",
    "    for i in range(len(u_series)):\n",
    "        mapper[u_series[i]] = i\n",
    "        \n",
    "    return mapper\n",
    "\n",
    "train_keyword_map = map_keywords(train.keyword)\n",
    "\n",
    "train['keyword_num'] = train['keyword'].map(train_keyword_map)\n",
    "test['keyword_num'] = test['keyword'].map(train_keyword_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(data):\n",
    "    #remove whitespace and lower all words\n",
    "    data = data.apply(lambda x: x.strip().lower())\n",
    "    #replace digits\n",
    "    data = data.apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "    #replace punctuation\n",
    "    data = data.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "    #tokenize\n",
    "    data = data.apply(lambda x : word_tokenize(x))\n",
    "    #filter out stopwords\n",
    "    data = data.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    #remove inflection and return base word\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data = data.apply(lambda x: [lemmatizer.lemmatize(word, pos ='v') for word in x])\n",
    "    #parts of speech tagging\n",
    "    #data = data.apply(lambda x: [pos_tag(x)])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pro_text'] = text_preprocessing(train.text)\n",
    "test['pro_text'] = text_preprocessing(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>keyword_num</th>\n",
       "      <th>pro_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[residents, ask, shelter, place, notify, offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[get, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  keyword_num                                           pro_text  \n",
       "0       1            0  [deeds, reason, earthquake, may, allah, forgiv...  \n",
       "1       1            0      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2       1            0  [residents, ask, shelter, place, notify, offic...  \n",
       "3       1            0  [people, receive, wildfires, evacuation, order...  \n",
       "4       1            0  [get, send, photo, ruby, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Vectorizer\n",
    "Matrix of token counts with TF-IDF transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 8037)\n",
      "(3263, 8037)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "#joining words and fit transofrming\n",
    "vector = vectorizer.fit_transform([\"\".join(i) for i in train[\"pro_text\"]])\n",
    "vector = vector.todense()\n",
    "vector = np.concatenate((vector, np.reshape(np.array(train[\"keyword_num\"]), (train.keyword.shape[0],-1))), axis=1)\n",
    "print(vector.shape)\n",
    "\n",
    "# vector_test = vectorizer.fit_transform([\"\".join(i) for i in test[\"text\"]])\n",
    "vector_test = vectorizer.transform([\"\".join(i) for i in test[\"pro_text\"]])\n",
    "vector_test = vector_test.todense()\n",
    "vector_test = np.concatenate((vector_test, np.reshape(np.array(test[\"keyword_num\"]), (test.keyword.shape[0],-1))), axis=1)\n",
    "print(vector_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.0, 0.0, 0.0, ..., 0.0, 0.0, 'deluged'],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 'violent%20storm'],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 'hazard'],\n",
       "        ...,\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 'landslide'],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 'deaths'],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 'detonation']], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(vector, train['target'], train_size = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.5724789915966386\n",
      "Precision score:  0.8888888888888888\n",
      "Recall score:  0.01932367149758454\n",
      "F1 score :  0.03782505910165485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1074,    2],\n",
       "       [ 812,   16]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "model = LinearSVC(loss=\"hinge\",fit_intercept=False, max_iter=1500)\n",
    "model = model.fit(xtrain, ytrain) \n",
    "predictions = model.predict(xtest)\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score(ytest, predictions))\n",
    "print(\"Precision score: \", precision_score(ytest, predictions))\n",
    "print(\"Recall score: \", recall_score(ytest, predictions))\n",
    "print(\"F1 score : \", f1_score(predictions, ytest))\n",
    "\n",
    "confusion_matrix(ytest, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy - TP + TN / (TP + TN + FP + FN)\n",
    "# precision - TP / (TP + FP)\n",
    "# recall - TP / (TP + FN)\n",
    "# f1 score - 2  * (precision * recall ) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.5787815126050421\n",
      "Precision score:  0.9333333333333333\n",
      "Recall score:  0.033816425120772944\n",
      "F1 score :  0.06526806526806526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1074,    2],\n",
       "       [ 800,   28]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_lr = LogisticRegression(penalty='l2')\n",
    "model_lr = model_lr.fit(xtrain, ytrain) \n",
    "predictions = model_lr.predict(xtest)\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score(ytest, predictions))\n",
    "print(\"Precision score: \", precision_score(ytest, predictions))\n",
    "print(\"Recall score: \", recall_score(ytest, predictions))\n",
    "print(\"F1 score : \", f1_score(predictions, ytest))\n",
    "\n",
    "confusion_matrix(ytest, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.6685924369747899\n",
      "Precision score:  0.7296037296037297\n",
      "Recall score:  0.3780193236714976\n",
      "F1 score :  0.49801113762927607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[960, 116],\n",
       "       [515, 313]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model_xgb = XGBClassifier(metric= 'euclidean', n_neighbors= 3, weights= 'uniform')\n",
    "model_xgb = model_xgb.fit(xtrain, ytrain) \n",
    "predictions = model_xgb.predict(xtest)\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score(ytest, predictions))\n",
    "print(\"Precision score: \", precision_score(ytest, predictions))\n",
    "print(\"Recall score: \", recall_score(ytest, predictions))\n",
    "print(\"F1 score : \", f1_score(predictions, ytest))\n",
    "\n",
    "confusion_matrix(ytest, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.6465336134453782\n",
      "Precision score:  0.6962025316455697\n",
      "Recall score:  0.3321256038647343\n",
      "F1 score :  0.4497138184791496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[956, 120],\n",
       "       [553, 275]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "tree = DecisionTreeClassifier(random_state = 11, max_features = \"auto\", class_weight = \"balanced\",max_depth = None)\n",
    "\n",
    "model_ada = AdaBoostClassifier(base_estimator=tree)\n",
    "model_ada = model_ada.fit(xtrain, ytrain)\n",
    "predictions = model_ada.predict(xtest)\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score(ytest, predictions))\n",
    "print(\"Precision score: \", precision_score(ytest, predictions))\n",
    "print(\"Recall score: \", recall_score(ytest, predictions))\n",
    "print(\"F1 score : \", f1_score(predictions, ytest))\n",
    "\n",
    "confusion_matrix(ytest, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.5651260504201681\n",
      "Precision score:  0.0\n",
      "Recall score:  0.0\n",
      "F1 score :  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1076,    0],\n",
       "       [ 828,    0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gb = GradientBoostingClassifier(criterion='friedman_mse', learning_rate= 0.15, \n",
    "                                   loss= 'deviance', max_depth= 8, max_features='sqrt', \n",
    "                                   min_samples_leaf= 0.15714285714285714, min_samples_split= 0.5, \n",
    "                                   n_estimators= 10, subsample=1.0)\n",
    "model_gb = model_gb.fit(xtrain, ytrain)\n",
    "predictions = model_gb.predict(xtest)\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score(ytest, predictions))\n",
    "print(\"Precision score: \", precision_score(ytest, predictions))\n",
    "print(\"Recall score: \", recall_score(ytest, predictions))\n",
    "print(\"F1 score : \", f1_score(predictions, ytest))\n",
    "\n",
    "confusion_matrix(ytest, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_hub' has no attribute 'KerasLayer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_hub' has no attribute 'KerasLayer'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_hub' has no attribute 'KerasLayer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-8aa08a2cba0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_hub' has no attribute 'KerasLayer'"
     ]
    }
   ],
   "source": [
    "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
